{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTmNj6fHg6GyPQHdSdojg+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d286eb0b24e544698eee121560dd1a13":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Input:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_a34bd8bfa5cc4ed5a391b3c3c8a2e131","placeholder":"Type your input and press Enter","style":"IPY_MODEL_86f4949f8f6b4917b8c1773009f40bb3","value":"send money to daru ki dukan"}},"a34bd8bfa5cc4ed5a391b3c3c8a2e131":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86f4949f8f6b4917b8c1773009f40bb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers datasets torch scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EPzepDgcLhg2","executionInfo":{"status":"ok","timestamp":1753272900786,"user_tz":-330,"elapsed":78048,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"28647125-ef3e-4e6f-cd14-282f092d8650"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.7.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import BertTokenizer, AutoTokenizer\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import files\n","uploaded = files.upload()\n","\n","# Load and preprocess the dataset\n","df = pd.read_csv(\"final_dataset_make_call_unique_clean.csv\")  # Your dataset file\n","\n","# Convert BIO labels and intent labels to numeric\n","intent_labels = {'money_transfer': 0, 'out_of_scope': 1, 'make_call': 2, 'affirmative': 3, 'negative': 4, 'greeting': 5, 'more_options': 6, 'angry': 7}\n","\n","entity_labels_dict = {\n","    'O': 0,\n","    'B-amount': 1,\n","    'I-amount': 2,\n","    'B-contact_name': 3,\n","    'I-contact_name': 4,\n","    'B-payment_platform': 5,\n","    'I-payment_platform': 6\n","}\n","\n","\n","def process_row(row):\n","    tokens = row[2::2]  # Tokens are in alternate columns\n","    labels = row[3::2]  # BIO labels\n","    # Convert tokens to strings and handle NaN values\n","    tokens = [str(token) if pd.notna(token) else \"\" for token in tokens]\n","    # Convert labels to numeric values using the global entity_labels dictionary\n","    numeric_labels = [entity_labels_dict.get(label, 0) if pd.notna(label) else 0 for label in labels] # Use entity_labels_dict instead of entity_labels\n","\n","    return tokens, numeric_labels\n","\n","data = []\n","for _, row in df.iterrows():\n","    #print(f\"Total data: {data}\")\n","    tokens, entity_labels = process_row(row)\n","    data.append({\n","        \"text\": \" \".join(tokens),\n","        \"tokens\": tokens,\n","        \"intent\": intent_labels[row[\"Intent\"]],\n","        \"entity_labels\": entity_labels\n","    })\n","\n","from transformers import BertModel\n","import torch.nn as nn\n","import torch\n","\n","class BertForIntentAndEntities(nn.Module):\n","    def __init__(self, pretrained_model_name, num_intent_labels, num_entity_labels):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained(pretrained_model_name)\n","        self.dropout = nn.Dropout(0.1)  # <-- Added dropout\n","        self.intent_classifier = nn.Linear(self.bert.config.hidden_size, num_intent_labels)\n","        self.entity_classifier = nn.Linear(self.bert.config.hidden_size, num_entity_labels)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None, intent_labels=None):\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        sequence_output = self.dropout(outputs.last_hidden_state)   # <-- Added dropout here\n","        pooled_output = self.dropout(outputs.pooler_output)\n","\n","        intent_logits = self.intent_classifier(pooled_output)\n","        entity_logits = self.entity_classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None and intent_labels is not None:\n","            intent_loss_fn = nn.CrossEntropyLoss()\n","            entity_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # <-- ignores padding tokens\n","\n","            intent_loss = intent_loss_fn(intent_logits, intent_labels)\n","            entity_loss = entity_loss_fn(entity_logits.view(-1, entity_logits.shape[-1]), labels.view(-1))\n","\n","            # <-- CHANGE: give more weight to entity loss\n","            loss = 0.3 * intent_loss + 0.7 * entity_loss\n","\n","        return {\"loss\": loss, \"intent_logits\": intent_logits, \"entity_logits\": entity_logits}\n","\n","\n","# Split into train/test sets\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","# Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True) # Load the fast tokenizer using AutoTokenizer\n","\n","def tokenize_and_align_labels(example):\n","    # Tokenize the input tokens\n","    tokenized_inputs = tokenizer(\n","        example[\"tokens\"],\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=20,\n","        is_split_into_words=True,\n","        return_tensors=\"pt\"\n","    )\n","    # Align labels with the tokenized word IDs\n","    word_ids = tokenized_inputs.word_ids()\n","    # labels = [-100 if word_id is None else example[\"entity_labels\"][word_id] for word_id in word_ids]\n","    previous_word_id = None\n","    labels = []\n","\n","    for word_id in word_ids:\n","        if word_id is None:\n","            labels.append(-100)\n","        elif word_id != previous_word_id:\n","            labels.append(example[\"entity_labels\"][word_id])  # First subword gets label\n","        else:\n","            labels.append(-100)  # Subword — ignore in loss\n","        previous_word_id = word_id\n","    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n","    tokenized_inputs[\"intent_labels\"] = torch.tensor(example[\"intent\"])\n","\n","    # Print original text and tokenized inputs\n","    original_text = \" \".join(example[\"tokens\"])  # Combine tokens into a single text string\n","    # print(f\"Original Text: {original_text}\")\n","    # print(f\"Tokenized inputs: {tokenized_inputs}\")\n","\n","    return tokenized_inputs\n","\n","train_dataset = [tokenize_and_align_labels(example) for example in train_data]\n","val_dataset = [tokenize_and_align_labels(example) for example in val_data]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"DqBbjbWkLk-a","executionInfo":{"status":"ok","timestamp":1753273617828,"user_tz":-330,"elapsed":9732,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"8a45fdf9-cc12-4013-b700-44d95ea6593f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-3fbf416d-3ba0-4b45-8314-f16ea65010fc\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3fbf416d-3ba0-4b45-8314-f16ea65010fc\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving final_dataset_make_call_unique_clean.csv to final_dataset_make_call_unique_clean (1).csv\n"]}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n","\n","# Define the dataset class\n","class TokenizedDataset(Dataset):\n","    def __init__(self, tokenized_data):\n","        self.tokenized_data = tokenized_data\n","\n","    def __len__(self):\n","        return len(self.tokenized_data[\"input_ids\"])\n","\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.tokenized_data.items()}\n","\n","# Convert tokenized data to dictionary format\n","train_data = {\n","    \"input_ids\": [example[\"input_ids\"].squeeze() for example in train_dataset],\n","    \"attention_mask\": [example[\"attention_mask\"].squeeze() for example in train_dataset],\n","    \"labels\": [example[\"labels\"].squeeze() for example in train_dataset],  # Wrap labels list in torch.tensor()\n","    \"intent_labels\": [example[\"intent_labels\"].squeeze() for example in train_dataset]\n","}\n","val_data = {\n","    \"input_ids\": [example[\"input_ids\"].squeeze() for example in val_dataset],\n","    \"attention_mask\": [example[\"attention_mask\"].squeeze() for example in val_dataset],\n","    \"labels\": [example[\"labels\"].squeeze() for example in val_dataset],  # Wrap labels list in torch.tensor()\n","    \"intent_labels\": [example[\"intent_labels\"].squeeze() for example in val_dataset]\n","}\n","\n","# Wrap into PyTorch Dataset\n","train_dataset = TokenizedDataset(train_data)\n","val_dataset = TokenizedDataset(val_data)\n","\n","# Create DataLoaders\n","batch_size = 8\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)"],"metadata":{"id":"EfgOfFlmpbKF","executionInfo":{"status":"ok","timestamp":1753274171719,"user_tz":-330,"elapsed":11,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e07dee66-7e3f-4a77-e158-dd24ebde8dd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-25-2365521852.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return {key: torch.tensor(val[idx]) for key, val in self.tokenized_data.items()}\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.optim import AdamW # Import AdamW from torch.optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","from transformers import BertForTokenClassification\n","\n","# Load model\n","num_intent_labels = len(intent_labels)\n","num_entity_labels = len(entity_labels_dict)\n","model = BertForIntentAndEntities(\"bert-base-uncased\", num_intent_labels, num_entity_labels)\n","\n","\n","# Move model to the appropriate device (GPU or CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kXXIwasYt3Al","executionInfo":{"status":"ok","timestamp":1753274176263,"user_tz":-330,"elapsed":62,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"9c8459b9-c979-4039-8388-7ee2c45a63bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForIntentAndEntities(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (intent_classifier): Linear(in_features=768, out_features=8, bias=True)\n","  (entity_classifier): Linear(in_features=768, out_features=7, bias=True)\n",")"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from torch.optim import AdamW\n","\n","# Define the optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)"],"metadata":{"id":"GUfldKfH-UkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.utils import clip_grad_norm_\n","\n","# Training loop\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in tqdm(train_loader):\n","        # Move batch to device\n","        batch = {key: val.to(device) for key, val in batch.items()}\n","\n","        # Forward pass\n","        outputs = model(\n","            input_ids=batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            labels=batch[\"labels\"],\n","            intent_labels=batch[\"intent_labels\"]\n","        )\n","\n","        # Compute loss\n","        loss = outputs[\"loss\"]\n","        total_loss += loss.item()\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Gradient clipping\n","        clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        # Optimizer step\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    # Print epoch loss\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFj7JGUM-X7n","executionInfo":{"status":"ok","timestamp":1753274188622,"user_tz":-330,"elapsed":5069,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"7d64128d-6a52-447e-8c77-3b97515ad1f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/9 [00:00<?, ?it/s]/tmp/ipython-input-30-2365521852.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return {key: torch.tensor(val[idx]) for key, val in self.tokenized_data.items()}\n","100%|██████████| 9/9 [00:00<00:00, 11.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 1.6869\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, Loss: 1.1527\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10, Loss: 0.8949\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/10, Loss: 0.7437\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10, Loss: 0.5660\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/10, Loss: 0.4530\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/10, Loss: 0.3643\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/10, Loss: 0.2900\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/10, Loss: 0.2386\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<00:00, 13.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/10, Loss: 0.2129\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, accuracy_score\n","\n","# Evaluation loop\n","model.eval()  # Set the model to evaluation mode\n","all_entity_preds, all_entity_labels = [], []\n","all_intent_preds = []  # Initialize as empty list\n","all_intent_labels = []  # Initialize as empty list\n","\n","with torch.no_grad():  # Disable gradient computation\n","    for batch in val_loader:\n","        # Move batch to device\n","        batch = {key: val.to(device) for key, val in batch.items()}\n","\n","        # Forward pass\n","        outputs = model(\n","            input_ids=batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"]\n","        )\n","\n","        # Extract intent logits and predictions\n","        intent_logits = outputs[\"intent_logits\"]\n","        intent_predictions = torch.argmax(intent_logits, dim=1)  # Intent predictions\n","        all_intent_preds.extend(intent_predictions.cpu().numpy())\n","        all_intent_labels.extend(batch[\"intent_labels\"].cpu().numpy())\n","\n","        # Extract entity logits and predictions\n","        entity_logits = outputs[\"entity_logits\"]\n","        entity_predictions = torch.argmax(entity_logits, dim=2)  # Entity predictions\n","\n","        # Store entity predictions and labels\n","        all_entity_preds.extend(entity_predictions.cpu().numpy())\n","        all_entity_labels.extend(batch[\"labels\"].cpu().numpy())\n","\n","\n","# Flatten and filter out padding tokens (-100) for entities\n","entity_preds_flat = []\n","entity_labels_flat = []\n","\n","for pred, label in zip(all_entity_preds, all_entity_labels):\n","    for p, l in zip(pred, label):\n","        if l != -100:  # Ignore padding tokens\n","            entity_preds_flat.append(p)\n","            entity_labels_flat.append(l)\n","\n","# Identify the unique intent labels in the validation data\n","unique_labels = sorted(set(all_intent_labels))  # Unique labels in true data\n","\n","# Filter target names for available labels\n","available_target_names = [key for key, val in intent_labels.items() if val in unique_labels]\n","\n","#Generate intent classification report with only available labels\n","print(\"Intent Classification Report:\")\n","print(\n","    classification_report(\n","        all_intent_labels,\n","        all_intent_preds,\n","        target_names=available_target_names,\n","        labels=unique_labels\n","    )\n",")\n","\n","# Generate entity extraction report\n","print(\"\\nEntity Extraction Report:\")\n","print(classification_report(entity_labels_flat, entity_preds_flat))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMzY0QvxJR_o","executionInfo":{"status":"ok","timestamp":1753274191644,"user_tz":-330,"elapsed":51,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"3f3bbabb-3746-4b44-97d2-ea78f37a65da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Intent Classification Report:\n","                precision    recall  f1-score   support\n","\n","money_transfer       1.00      1.00      1.00         4\n","  out_of_scope       1.00      1.00      1.00         1\n","     make_call       1.00      1.00      1.00         7\n","   affirmative       1.00      1.00      1.00         1\n","      negative       1.00      1.00      1.00         1\n","      greeting       0.50      1.00      0.67         1\n","         angry       1.00      0.50      0.67         2\n","\n","      accuracy                           0.94        17\n","     macro avg       0.93      0.93      0.90        17\n","  weighted avg       0.97      0.94      0.94        17\n","\n","\n","Entity Extraction Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.94      0.96        53\n","           1       1.00      0.75      0.86         4\n","           2       0.80      1.00      0.89         4\n","           3       0.79      1.00      0.88        11\n","           4       1.00      1.00      1.00        11\n","           5       1.00      0.50      0.67         2\n","\n","    accuracy                           0.94        85\n","   macro avg       0.93      0.87      0.88        85\n","weighted avg       0.95      0.94      0.94        85\n","\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-30-2365521852.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return {key: torch.tensor(val[idx]) for key, val in self.tokenized_data.items()}\n"]}]},{"cell_type":"code","source":["import os\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(\"./trained_model\", exist_ok=True)\n","\n","# Save the model's state dictionary\n","torch.save(model.state_dict(), \"./trained_model/pytorch_model.bin\")\n","\n","entity_labels_dict = {\n","    'O': 0,\n","    'B-amount': 1,\n","    'I-amount': 2,\n","    'B-contact_name': 3,\n","    'I-contact_name': 4,\n","    'B-payment_platform': 5,\n","    'I-payment_platform': 6\n","}\n","\n","# Save the model configuration\n","model_config = {\n","    \"pretrained_model_name\": \"bert-base-uncased\",\n","    \"num_intent_labels\": len(intent_labels),\n","    \"num_entity_labels\": len(entity_labels_dict)\n","}\n","import json\n","with open(\"./trained_model/config.json\", \"w\") as f:\n","    json.dump(model_config, f)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(\"./trained_model\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YcAa-sK1PKH0","executionInfo":{"status":"ok","timestamp":1753274322142,"user_tz":-330,"elapsed":6595,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"f09c575a-e04e-4c29-fa9a-ec72559d5264"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('./trained_model/tokenizer_config.json',\n"," './trained_model/special_tokens_map.json',\n"," './trained_model/vocab.txt',\n"," './trained_model/added_tokens.json',\n"," './trained_model/tokenizer.json')"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# Original model checking code\n","import json\n","from transformers import BertModel, BertTokenizer\n","\n","# Load the model configuration\n","with open(\"./trained_model/config.json\", \"r\") as f:\n","    model_config = json.load(f)\n","\n","# Reinitialize the model\n","model = BertForIntentAndEntities(\n","    pretrained_model_name=model_config[\"pretrained_model_name\"],\n","    num_intent_labels=model_config[\"num_intent_labels\"],\n","    num_entity_labels=model_config[\"num_entity_labels\"]\n",")\n","\n","# Load the model's state dictionary\n","model.load_state_dict(torch.load(\"./trained_model/pytorch_model.bin\"))\n","\n","# Load the tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"./trained_model\")\n","\n","# Move to device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"ew0kLqReRGht","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735964223706,"user_tz":-330,"elapsed":1914,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"4040bed4-9ea9-4ad5-d9e7-fc9ca9ee3ecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-c7fd62265be2>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(\"./trained_model/pytorch_model.bin\"))\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForIntentAndEntities(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (intent_classifier): Linear(in_features=768, out_features=8, bias=True)\n","  (entity_classifier): Linear(in_features=768, out_features=7, bias=True)\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# drive model checking code\n","\n","\n","from transformers import BertModel\n","import torch.nn as nn\n","import torch\n","\n","class BertForIntentAndEntities(nn.Module):\n","    def __init__(self, pretrained_model_name, num_intent_labels, num_entity_labels):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained(pretrained_model_name)\n","        self.dropout = nn.Dropout(0.1)  # <-- Added dropout\n","        self.intent_classifier = nn.Linear(self.bert.config.hidden_size, num_intent_labels)\n","        self.entity_classifier = nn.Linear(self.bert.config.hidden_size, num_entity_labels)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None, intent_labels=None):\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        sequence_output = self.dropout(outputs.last_hidden_state)   # <-- Added dropout here\n","        pooled_output = self.dropout(outputs.pooler_output)\n","\n","        intent_logits = self.intent_classifier(pooled_output)\n","        entity_logits = self.entity_classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None and intent_labels is not None:\n","            intent_loss_fn = nn.CrossEntropyLoss()\n","            entity_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # <-- ignores padding tokens\n","\n","            intent_loss = intent_loss_fn(intent_logits, intent_labels)\n","            entity_loss = entity_loss_fn(entity_logits.view(-1, entity_logits.shape[-1]), labels.view(-1))\n","\n","            # <-- CHANGE: give more weight to entity loss\n","            loss = 0.3 * intent_loss + 0.7 * entity_loss\n","\n","        return {\"loss\": loss, \"intent_logits\": intent_logits, \"entity_logits\": entity_logits}\n","\n","import os\n","\n","# Path to your saved model in Drive\n","model_dir = \"/content/drive/MyDrive/model_folder_unique\"\n","\n","import json\n","from transformers import BertModel, BertTokenizer\n","\n","# Load the model configuration\n","with open(os.path.join(model_dir, \"config.json\"), \"r\") as f:\n","    model_config = json.load(f)\n","\n","# Reinitialize the model\n","model = BertForIntentAndEntities(\n","    pretrained_model_name=model_config[\"pretrained_model_name\"],\n","    num_intent_labels=model_config[\"num_intent_labels\"],\n","    num_entity_labels=model_config[\"num_entity_labels\"]\n",")\n","\n","# Load the model's state dictionary\n","model.load_state_dict(torch.load(os.path.join(model_dir, \"pytorch_model.bin\"), map_location=torch.device('cpu')))\n","\n","# Load the tokenizer\n","tokenizer = BertTokenizer.from_pretrained(model_dir)\n","\n","# Move to device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C9x3OUoTKm_F","executionInfo":{"status":"ok","timestamp":1753361989324,"user_tz":-330,"elapsed":1795,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"21dd9038-ed27-4bb3-8988-baa8c7c1e61a"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForIntentAndEntities(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (intent_classifier): Linear(in_features=768, out_features=8, bias=True)\n","  (entity_classifier): Linear(in_features=768, out_features=7, bias=True)\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["def predict_intent_and_entities(text):\n","    import re\n","    # Input text\n","\n","    # Tokenize the input\n","    tokens = tokenizer(\n","        text.split(),\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=20,\n","        is_split_into_words=True,\n","        return_tensors=\"pt\"\n","    )\n","    #print(tokens)\n","\n","    tokens = {key: val.to(device) for key, val in tokens.items()}\n","\n","    #print(tokens)\n","\n","    # Perform inference\n","    with torch.no_grad():\n","        outputs = model(**tokens)\n","\n","    # Get logits for intent and entities\n","    entity_logits = outputs[\"entity_logits\"]  # Entity logits\n","    intent_logits = outputs[\"intent_logits\"]  # Intent logits\n","\n","    # Intent prediction\n","    intent_prediction = torch.argmax(intent_logits, dim=1)\n","    intent_labels = {\n","        0: 'money_transfer',\n","        1: 'out_of_scope',\n","        2: 'make_call',\n","        3: 'affirmative',\n","        4: 'negative',\n","        5: 'greeting',\n","        6: 'more_options',\n","        7: 'angry'\n","    }\n","    predicted_intent = intent_labels[intent_prediction.item()]\n","    print(f\"Predicted Intent: {predicted_intent}\")\n","\n","    # Entity prediction\n","    entity_predictions = torch.argmax(entity_logits, dim=2)\n","    #print(entity_predictions)\n","\n","\n","    # Mapping from label ID to label name\n","    id2label = {\n","        0: 'O',\n","        1: 'B-amount',\n","        2: 'I-amount',\n","        3: 'B-contact_name',\n","        4: 'I-contact_name',\n","        5: 'B-payment_platform',\n","        6: 'I-payment_platform'\n","    }\n","\n","    # Convert predictions to labels\n","    predicted_labels = [id2label[label_id] for label_id in entity_predictions[0].cpu().numpy()]\n","    #print(predicted_labels)\n","\n","    # Convert token IDs to tokens\n","    tokens_text = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n","    #print(tokens_text)\n","\n","    # Combine tokens with their predicted labels\n","    results = [(token, label) for token, label in zip(tokens_text, predicted_labels) if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]]\n","\n","    # Print results\n","    for token, label in results:\n","        print(f\"{token:15} -> {label}\")\n","\n","    entities = {}\n","    current_entity = None\n","    entity_tokens = []\n","\n","    for token, label in results:\n","        print(f\"Token: {token}, Label: {label}\")\n","        if label.startswith(\"B-\"):  # Beginning of a new entity\n","            if current_entity:\n","                if current_entity == label[2:]:\n","                    if token.startswith(\"##\"):\n","                        # Subword continuation: merge into previous token\n","                        entity_tokens[-1] += token[2:]  # Append subword without \"##\"\n","                    else:\n","                        entity_tokens.append(token)  # Add new token\n","                else:\n","                    # Add the completed entity to the result\n","                    entities[current_entity] = \" \".join(entity_tokens)\n","                    current_entity = None\n","                    entity_tokens = []\n","                    entity_type = label[2:]  # Extract entity type (e.g., 'amount')\n","                    current_entity = entity_type\n","                    entity_tokens = [token]  # Start a new entity\n","            else:\n","              entity_type = label[2:]  # Extract entity type (e.g., 'amount')\n","              current_entity = entity_type\n","              entity_tokens = [token]  # Start a new entity\n","        elif label.startswith(\"I-\") and current_entity == label[2:]:  # Continuation of the same entity\n","            if token.startswith(\"##\"):\n","                # Subword continuation: merge into previous token\n","                entity_tokens[-1] += token[2:]  # Append subword without \"##\"\n","            else:\n","                entity_tokens.append(token)  # Add new token\n","        else:\n","            if current_entity:\n","                # Add the completed entity to the result\n","                entities[current_entity] = \" \".join(entity_tokens)\n","            current_entity = None\n","            entity_tokens = []\n","\n","        # Print the variables at each iteration\n","        # print(f\"Current Entity: {current_entity}\")\n","        # print(f\"Entity Tokens: {entity_tokens}\")\n","        # print(f\"Entities: {entities}\")\n","        # print(\"-\" * 40)  # Separator for readability\n","\n","    # Add the last entity (if any)\n","    if current_entity:\n","        entities[current_entity] = \" \".join(entity_tokens)\n","\n","    print(\"Final Extracted Entities:\", entities)\n","\n","    # Check if 'amount' key exists in the entities\n","    if 'amount' in entities:\n","        numbers = re.findall(r'\\d+', entities['amount'])\n","        if \"one\" in entities['amount']:\n","            if numbers:\n","                total_amount = sum(map(int, numbers)) + 1\n","            else:\n","                total_amount = 1\n","        else:\n","            total_amount = sum(map(int, numbers)) if numbers else 0\n","        entities['amount'] = str(total_amount)\n","\n","    print(\"Updated Entities:\", entities)\n","\n"],"metadata":{"id":"pS8oO42fNWvV","executionInfo":{"status":"ok","timestamp":1753363830564,"user_tz":-330,"elapsed":358,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# prompt: delete trained_model folder with all file within\n","\n","import shutil\n","import os\n","\n","# Check if the directory exists before attempting to remove it\n","if os.path.exists(\"./trained_model\"):\n","  shutil.rmtree(\"./trained_model\")\n","  print(\"Directory 'trained_model' and its contents removed successfully.\")\n","else:\n","  print(\"Directory 'trained_model' does not exist.\")"],"metadata":{"id":"JsUiCM4pHECS","executionInfo":{"status":"ok","timestamp":1753362000699,"user_tz":-330,"elapsed":447,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0c818b8-5b8c-4523-cb77-8f954114dbef"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Directory 'trained_model' does not exist.\n"]}]},{"cell_type":"code","source":["from IPython.display import display, clear_output\n","import ipywidgets as widgets\n","\n","# Create input box\n","text_box = widgets.Text(\n","    value='',\n","    placeholder='Type your input and press Enter',\n","    description='Input:',\n","    disabled=False\n",")\n","\n","# Define the function to call on Enter key press\n","def on_enter_pressed(sender):\n","    clear_output(wait=True)  # Clear previous output\n","    display(text_box)        # Redisplay the input box\n","    text = sender.value\n","    predict_intent_and_entities(text)  # Your prediction function\n","\n","# Bind the event\n","text_box.on_submit(on_enter_pressed)\n","\n","# Display the input box initially\n","display(text_box)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393,"referenced_widgets":["d286eb0b24e544698eee121560dd1a13","a34bd8bfa5cc4ed5a391b3c3c8a2e131","86f4949f8f6b4917b8c1773009f40bb3"]},"id":"sttYw7I3iCEy","executionInfo":{"status":"ok","timestamp":1753363840966,"user_tz":-330,"elapsed":21,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"8cb35df8-910e-489b-baa7-ba5669e62a0a"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["Text(value='send money to daru ki dukan', description='Input:', placeholder='Type your input and press Enter')"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d286eb0b24e544698eee121560dd1a13"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Predicted Intent: money_transfer\n","send            -> O\n","money           -> O\n","to              -> O\n","dar             -> B-contact_name\n","##u             -> I-contact_name\n","ki              -> I-contact_name\n","du              -> B-contact_name\n","##kan           -> I-contact_name\n","Token: send, Label: O\n","Token: money, Label: O\n","Token: to, Label: O\n","Token: dar, Label: B-contact_name\n","Token: ##u, Label: I-contact_name\n","Token: ki, Label: I-contact_name\n","Token: du, Label: B-contact_name\n","Token: ##kan, Label: I-contact_name\n","Final Extracted Entities: {'contact_name': 'daru ki dukan'}\n","Updated Entities: {'contact_name': 'daru ki dukan'}\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqp809hiR9Sr","executionInfo":{"status":"ok","timestamp":1753357929876,"user_tz":-330,"elapsed":33125,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"outputId":"ab5a2edb-54fa-43fb-b477-6035cb060315"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import shutil\n","\n","# Path to the source zip file (in Colab workspace)\n","source_zip = \"trained_model\"\n","\n","# Path to save in Google Drive\n","destination_zip = \"/content/drive/MyDrive/model_folder_unique\"\n","destination = '/content/drive/MyDrive/model_folder_unique.zip'\n","\n","if os.path.exists(destination):\n","  os.remove(destination)\n","  print(f\"File '{destination}' deleted successfully.\")\n","else:\n","  print(f\"File '{destination}' does not exist.\")\n","\n","if os.path.exists(destination_zip):\n","  # If it exists, remove it to avoid the FileExistsError\n","  shutil.rmtree(destination_zip)\n","  print(f\"Existing directory '{destination_zip}' removed.\")\n","else:\n","  print(f\"Directory '{destination_zip}' does not exist.\")\n","\n","# Move the file to Google Drive\n","shutil.copytree(source_zip, destination_zip)\n","\n","print(f\"File moved to Google Drive at {destination_zip}\")\n","\n","# Replace 'my_folder' with your folder name and 'my_folder.zip' with the desired zip name\n","shutil.make_archive('/content/drive/MyDrive/model_folder_unique', 'zip', '/content/drive/MyDrive/model_folder_unique')\n","\n","print(f\"File moved to Google Drive at {destination_zip}\")\n","shutil.copy(destination, '/content/')"],"metadata":{"id":"sLq448WMSCpu","executionInfo":{"status":"ok","timestamp":1753358347689,"user_tz":-330,"elapsed":35604,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"a934efb2-2dc9-4f74-a4b1-26432eaad0f7"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/model_folder_unique.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["!apt-get install openssh-client\n","from google.colab import files\n","uploaded = files.upload()\n","!mkdir -p ~/.ssh # if already created then......\n","!chmod 700 ~/.ssh\n","!chmod 400 /content/aws_ec2_key.pem\n","!ssh-keyscan -H 13.232.75.140 >> ~/.ssh/known_hosts"],"metadata":{"id":"ANWtGOL9NraP","executionInfo":{"status":"ok","timestamp":1753358824467,"user_tz":-330,"elapsed":32807,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}},"colab":{"base_uri":"https://localhost:8080/","height":254},"outputId":"bb926a3f-3a82-44ce-c2e5-e301f18f6601"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","openssh-client is already the newest version (1:8.9p1-3ubuntu0.13).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-20fef9fa-f4ef-4369-8d4f-d693355c7761\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-20fef9fa-f4ef-4369-8d4f-d693355c7761\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving aws_ec2_key.pem to aws_ec2_key (1).pem\n","# 13.232.75.140:22 SSH-2.0-OpenSSH_8.7\n","# 13.232.75.140:22 SSH-2.0-OpenSSH_8.7\n","# 13.232.75.140:22 SSH-2.0-OpenSSH_8.7\n","# 13.232.75.140:22 SSH-2.0-OpenSSH_8.7\n","# 13.232.75.140:22 SSH-2.0-OpenSSH_8.7\n"]}]},{"cell_type":"code","source":["!scp -i aws_ec2_key.pem /content/drive/MyDrive/model_folder_unique.zip ec2-user@13.232.75.140:/home/ec2-user/trained_model/"],"metadata":{"id":"2oOBsgvsKQkk","executionInfo":{"status":"ok","timestamp":1753358977752,"user_tz":-330,"elapsed":58037,"user":{"displayName":"Tanmay Manna","userId":"01092653032338028221"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ce512yIBVeXq"},"execution_count":null,"outputs":[]}]}